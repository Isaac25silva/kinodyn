\documentclass[letterpaper, 10pt, english, conference]{IEEEtran}

\overrideIEEEmargins

\usepackage{amsfonts}
\usepackage{amsmath,amssymb}
\usepackage[tight]{subfigure}
\usepackage{cite}
\usepackage[vlined,linesnumbered,ruled]{algorithm2e}
\usepackage{color}

\ifx\pdfoutput\undefined
\usepackage{graphicx} % not running pdftex
\else
\ifx\pdfoutput\relax
\usepackage{graphicx} % not running pdftex
\else
\ifnum\pdfoutput>0
\usepackage[pdftex]{graphicx} % running pdftex with pdf output
\else
\usepackage{graphicx} % running pdftex with dvi output
\fi
\fi
\fi

\usepackage{verbatim}


%\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage[T1]{fontenc}
%\usepackage{pslatex}

%\makeatother

%\usepackage{babel}
\begin{document}

\global\long\def\at#1{}

\newcommand{\rrtstar}{RRT$^*$ }

\title{\LARGE \bf Optimal Sampling-Based Planning with
Automatically Derived Extension Procedures for Systems
with General Costs and Dynamics}

\author{
\authorblockN{Gustavo Goretkin, Alejandro Perez, Robert Platt Jr., and George Konidaris; TLPK?}
\authorblockA{Computer Science and Artificial Intelligence Laboratory,
Massachusetts Institute of Technology\\
\{goretkin, aperez, rplatt, gdk\}@csail.mit.edu}
}

\maketitle

\begin{abstract}
We introduce a method for automatically deriving the cost metric and extension
procedures--- critical components of the RRT$^*$ algorithm---for 
general systems with differentiable dynamics and
twice differentiable cost functions. 
%
We first introduce a 
version of the RRT$^*$ algorithm that uses an LQR-based
extension to
plan for problems with affine dynamics and second order costs,
and prove that it almost surely converges to the optimal plan.  
We then show how to automatically derive a cost metric and
extension method for problems with nonlinear dynamics and
non-quadratic costs by locally approximating both functions. This
allows us to automatically derive an extension method for 
general domains, provided their dynamics are locally differentiable  and
their cost function locally twice differentiable. This 
removes the need for
domain-specific  extension methods, which are a major
obstacle to the general applicability of RRT$^*$. 
We demonstrate
our method's application in a few standard benchmark domains. 
\end{abstract}

\section{Introduction}
The RRT$^*$ algorithm \cite{karaman.frazzoli.ijrr11} offers a practical,
effective method for probabilistically complete optimal motion planning.
However, like all members of the RRT family of planning algorithms
\cite{lavalle.kuffner.ijrr01}, it incrementally builds a tree using an extension procedure which
%given a pair of points, both estimates the optimal cost of moving from 
%one to the other, and provides a method for doing so.
requires a cost metric to determine closest points in the tree as well as a method for computing trajectories between pairs of points. While designing
such procedures is straightforward for kinematic systems, it is often difficult to design these for more complex systems as they must accurately reflect the dynamics of the domain. Since the 
performance of RRT-based algorithms is sensitive to the metric and extension procedure
\cite{LaVallefromdyn,Cheng01}, designing them is often the a major
obstacle to their application.

Recently, several authors have proposed using methods from linear
control theory to automatically derive all or part of the extension
procedure, as was originally suggested by LaValle and Kuffner
\cite{lavalle.kuffner.ijrr01}. Glassman and Tedrake \cite{elena.russ.icra10}
derived a cost-to-go pseudo-metric based on linear-quadratic regulators (LQR),
which was used to grow an RRT in domains with constrained and 
underactuated dynamics. Perez et al. \cite{Perez12} extended their work
to automatically derive the extension procedure for arbitrary systems
with differentiable dynamics and quadratic cost functions. Webb and van den Berg \cite{jur} proposed a method for computing trajectories between pairs of states by minimizing a cost function with a tunable trade-off between duration and control effort. ...

We propose a method for deriving the extension procedure for systems
with general costs and dynamics (provided the dynamics are differentiable
and the costs are twice differentiable). We begin by introducing a 
version of the RRT$^*$ algorithm that uses an LQR-based
extension to plan for problems with affine dynamics and second order costs,
and prove that it almost surely converges to the optimal plan.  
We then show how to automatically derive an
extension procedure for more general problems using
a local linear approximation to the dynamics and a local quadratic
approximation to the cost function. 
MAYBE: WE PROVE THAT OUR METHOD IS PROBABILISTICALLY COMPLETE AND
ALMOST SURELY CONVERGES TO THE OPTIMAL PLAN. THIS METHOD IS
THEREFORE CAPABLE OF FINDING SUCH A PLAN FOR ANY SYSTEM ... (CONDITIONS)
We demonstrate
our method's application in a few standard benchmark domains. 

%\rrtstar  
%
%\begin{comment}
%Explain the distinction between general differential constraints and
%kinodynamic constraints
%\end{comment}



\section{Background}

\subsection{Problem Statement}

Given a system with known process dynamics, the
optimal motion planning problem is to find a minimum cost feasible
trajectory from an initial configuration to a goal configuration. Let
{\em configuration space} be a compact set, $X \subseteq \mathcal{X}$,
that describes all possible configurations of the system. Let $U
\subseteq \mathcal{U}$ be a compact control input space. We assume that we
are given the process dynamics:
\[
x_{k+1} = f(x,u)
\]
A {\em dynamically feasible trajectory} is a continuous function,
$\sigma: [0,1] \to X$, for which there exists a control function,
$u:[0,1] \to U$, such that $\forall t \in [0,1], \dot{\sigma}(t) =
f(\sigma(t),u(t))$. The set of all dynamically feasible trajectories
is denoted by $\Sigma_\mathrm{traj}$. Given an initial configuration,
$x_\mathrm{init}$, and a goal region $X_\mathrm{goal}$, the motion
planning problem is to find a dynamically feasible trajectory,
$\sigma$, and a control, $u$, that starts in the initial configuration
and reaches the goal region: $\sigma(0) = x_\mathrm{init}$ and
$\sigma(1) \in X_\mathrm{goal}$. Let $c : \Sigma_\mathrm{traj} \to
\mathbb{R}^+$ be a {\em cost functional} that maps each feasible
trajectory to a non-negative cost. The optimal motion planning problem
is to find a solution to the motion planning problem that minimizes
$c(\sigma)$.

\subsection{RRT$^*$}

Optimal Rapidly-exploring random tree (RRT$^*$) \cite{karaman.frazzoli.ijrr11} is a version of the RRT algorithm~\cite{lavalle.kuffner.ijrr01} that has the \emph{asymptotic optimality} property, i.e., almost-sure convergence to an optimal solution.

The major components of the algorithm are:
\begin{itemize}
\item {\it Random sampling}:
%
The ${\tt Sample}$ procedure provides independent uniformly distributed random samples of states from the configuration space.

\item {\it Near nodes}:
% 
Given a set $V$ of vertices in the tree and a state $x$, the ${\tt
  Near}(V, x)$ procedure provides a set of states in $V$ that are close to
$x$. Here, closeness is measured with respect to some metric as
follows:
$$
{\tt Near}(V, x) := \left\{ x' \in V \,\,:\,\, \Vert x - x' \Vert \le \gamma
\left(\frac{\log n}{n}\right)^{1/d} \right\},
$$
where $\Vert \cdot \Vert$ is the distance using the metric, $n$ is the number of vertices in the tree, $d$ is the dimension of the space, and $\gamma$ is a constant.
 
\item {\it Steering}:
%
Given two states $x, x'$, the ${\tt Steer}(x,x')$ procedure returns a path $\sigma$ that connects $x$ and $x'$. Most implementations connect two given states with a straight path in configuration space.

\item {\it Collision checking}:
%
Given a path $\sigma$, ${\tt CollisionFree}(\sigma)$ returns true if the path lies in the obstacle-free portion of  configuration space.

\end{itemize}

\begin{center}
\begin{algorithm}
\For {$i =1, \ldots, N$} { \label{line:iteration_start_orig}
	$x_\mathrm{rand} \leftarrow {\tt Sample}$\; \label{line:rrtstar:sample}
	$X_\mathrm{near} \leftarrow \mathtt{Near}(V ,x_\mathrm{rand})$\; \label{line:rrtstar:compute_near_orig}
	$(x_\mathrm{min},\sigma_\mathrm{min}) \leftarrow {\tt ChooseParent}(X_\mathrm{near}, x_\mathrm{rand})$\; \label{line:rrtstar:call_find_min_cost_parent}
	\If{${\tt CollisionFree}(\sigma)$}{ \label{line:rrtstar:check_collision}
		$V \leftarrow V \cup \{ x_\mathrm{rand} \}$\;  
		$E \leftarrow E \cup \{ (x_\mathrm{min}, x_\mathrm{rand})$ \}\;
		$(V,E) \leftarrow {\tt Rewire}(\,(V,E), X_\mathrm{near}, x_\mathrm{rand} \,)$\;
	}
	}
%}
\Return {$G = (V,E)$}\;
\caption{${\tt RRT}^* ((V,E), N)$}
\label{algorithm:rrtstar_orig}
\end{algorithm}
\end{center}

\vspace*{-.4in}

\begin{center}
\begin{algorithm}
$\mathrm{minCost} \leftarrow \infty$;\,
$x_\mathrm{min} \leftarrow {\tt NULL}$;
$\sigma_\mathrm{min} \leftarrow {\tt NULL}$\;
\For{$x_\mathrm{near} \in X_\mathrm{near}$}{
	$\sigma \leftarrow {\tt Steer}(x_\mathrm{near}, x_\mathrm{rand})$\;
	\If {${\tt Cost}(x_\mathrm{near}) + {\tt Cost}(\sigma) < \mathrm{minCost}$} {
		$\mathrm{minCost} \leftarrow {\tt Cost}(x_\mathrm{near}) + {\tt Cost}(\sigma)$\;
		$x_\mathrm{min} \leftarrow x_\mathrm{near}$;
		$\sigma_\mathrm{min} \leftarrow \sigma$\;
	}
}
\Return{$(x_\mathrm{min},\sigma_\mathrm{min})$}\;
\caption{${\tt ChooseParent}(X_\mathrm{near}, x_\mathrm{rand})$}
\label{algorithm:find_parent_orig}
\end{algorithm}
\end{center}

\vspace*{-.4in}

\begin{center}
\begin{algorithm}[h!!!]
\For{$x_\mathrm{near} \in X_\mathrm{near}$} {
	$\sigma \leftarrow {\tt Steer}(x_\mathrm{rand}, x_\mathrm{near})$ \;
	\If{${\tt Cost}(x_\mathrm{rand}) + {\tt Cost}(\sigma) < {\tt Cost}(x_\mathrm{near})$}{
		\If {${\tt CollisionFree}(\sigma)$}{	
			$x_\mathrm{parent} \leftarrow {\tt Parent}(x_\mathrm{near})$\;
			$E \leftarrow E \setminus \{ x_\mathrm{parent}, x_\mathrm{near}\}$\;
			$E \leftarrow E \cup \{ x_\mathrm{rand}, x_\mathrm{near}\}$\;
		}
	}
}
\Return{$(V,E)$}\;
\caption{${\tt Rewire}( \, (V,E),X_\mathrm{near}, x_\mathrm{rand} \, )$}
\label{algorithm:rewire_orig}
\end{algorithm}
\end{center}
\vspace*{-.4in}

The algorithm proceeds as follows.
Firstly, a state is sampled, denoted as
$x_\mathrm{rand}$, from the configuration space
(Line~\ref{line:rrtstar:sample}), and near neighbors are evaluated
using the ${\tt Near}$ function
(Line~\ref{line:rrtstar:compute_near_orig}).\footnote{Although it is not
  explicitly noted here,
%  in the algorithm description in order not to
 % complicate the discussion, 
 we assume that the algorithm proceeds to
  the next iteration if the ${\tt Near}$ function returns an empty
  set.} Next, RRT$^*$ calls ${\tt ChooseParent}$
(Algorithm~\ref{algorithm:find_parent_orig}) to find a candidate for a
parent node to $x_\mathrm{rand}$
(Line~\ref{line:rrtstar:call_find_min_cost_parent}). ${\tt
  ChooseParent}$ searches through the nodes in the set
$X_\mathrm{near}$ of near nodes and returns the node, denoted as
$x_\mathrm{min}$, that reaches $x_\mathrm{rand}$ with minimum cost
along with the path $\sigma_\mathrm{min}$ connecting
$x_\mathrm{min}$ to $x_\mathrm{rand}$. After that node
%the node within
%$X_\mathrm{near}$ that reaches $x_\mathrm{rand}$ (possibly with a path that is {\em not}
%collision free) 
is found, the algorithm checks $\sigma_\mathrm{min}$ for collision
(Line~\ref{line:rrtstar:check_collision}). If the path
is obstacle-free,
%$\sigma_\mathrm{min}$ collides with an obstacle, then the algorithm
%proceeds with the next iteration. Otherwise, 
the algorithm adds
$x_\mathrm{rand}$ to the tree and connects $x_\mathrm{min}$ to
$x_\mathrm{rand}$, and attempts to ``rewire'' the nodes in
$X_\mathrm{near}$ using the ${\tt Rewire}$ procedure
(Algorithm~\ref{algorithm:rewire_orig}). The ${\tt Rewire}$
procedure attempts to connect $x_\mathrm{rand}$ with each node in the
set $X_\mathrm{near}$ of near nodes. If the path that connects
$x_\mathrm{rand}$ with a near node $x_\mathrm{near}$ reaches
$x_\mathrm{near}$ with cost less than that of its current parent, then
the $x_\mathrm{near}$ is ``rewired'' to $x_\mathrm{rand}$ by
connecting $x_\mathrm{rand}$ and $x_\mathrm{near}$.

\subsection{Math Tools}

Linear Quadratic Regulator (LQR) is an optimal control technique which
determines the policy (mapping from state to action) for a linear
dynamical system which minimizes a quadratic functional on the state
and action.

LQR is used to solve the following problem: 

Find $\left\langle u_{0},\cdots u_{T-1}\right\rangle $ such that 

\[
x_{k+1}=Ax_{k}+Bu_{k}
\]


minimizing 
\[
J\left(x,u\right)=\sum_{k=0}^{T-1}\left(x_{k}^{T}Qx_{k}+u_{k}^{T}Ru_{k}\right)+x_{T}^{T}Q_{T}x_{T}
\]


given $A$, $B$, $Q$, $R$, $x_{0}$, $x_{T}$, and $T$.

We can enforce that LQR policies obey the final-value constraint on
$x_{T}$ by placing very high cost (practically infinite) on the $Q_{T}$
fed to the finite-horizon LQR solver. The textbook LQR problem statement
requires that the quadratic bowl $x^{T}Qx$ be centered at $x=0$.
A shift of coordinates can change the center of all bowls, which is
not what we want.

We can enrich the class of LQR problems to have arbitrary second-order
penalty (along with first-order dynamics, which will be essential
later) by considering the follwing transformation:

\begin{align*}
\underbrace{\left[\begin{matrix}x_{k+1}\\
1
\end{matrix}\right]}_{\hat{x}_{k+1}} & =\underbrace{\left(\begin{matrix}A & c-R^{-1}r\\
0 & 1
\end{matrix}\right)}_{\hat{A}}\underbrace{\left[\begin{matrix}x_{k}\\
1
\end{matrix}\right]}_{\hat{x}_{k}}+\underbrace{\left(\begin{matrix}B\\
0
\end{matrix}\right)}_{\hat{B}}\hat{u}_{k}\\
J\left(x,u\right) & =\sum_{k=0}^{T-1}\hat{x}_{k}^{T}\hat{Q}\hat{x}_{k}+\hat{u}_{k}^{T}R\hat{u}_{k}+\hat{x}_{T}^{T}\hat{Q}_{T}\hat{x}_{T}
\end{align*}


with $\hat{u}_{k}=u_{k}+R^{-1}r$ and $\hat{Q}=\left(\begin{matrix}Q & q\\
q^{T} & d
\end{matrix}\right)$ . 

Here we augment the state vector with an additional dimension whose
value is constrained by the dynamics to remain constant. As long as
this additional dimension is unity in $x_{0}$ , then it will be unity
along all trajectories.

In addition to allowing first-order dynamics (increasing the class
of systems from linear to affine), this transformation also allows
an arbitrary second-order function on $x$ and $u$. For example,
say we want the following bowl:

\[
\left(x-x_{0}\right)^{T}Q\left(x-x_{0}\right)=x^{T}Qx+-2x_{0}^{T}Qx+x_{0}^{T}Qx_{0}
\]


Then choose $\hat{Q}=\left(\begin{array}{cc}
Q & -Qx_{0}\\
-x_{0}^{T}Q & x_{0}^{T}Qx_{0}
\end{array}\right)$.


\section{Constrained Linear Systems with Second-Order Cost Functions}

Necessary since in general, not possible or practical for steer function
to be exact. 

Logically equivalent to common formulation of RRT$^*$, but easier to
plug in the primitives.

Algorithm detailing the different structure here 

For this class of systems, the LQR method is the standard way to find
optimal solution. LQR, though, cannot handle constaints on the state
nor on actuation.

We can use LQR in the distance metric calculation.


\subsection{Augmenting State with Time}

Augmenting time in the \rrtstar state space allows us to set explicit
time goals and allows us to apply the LQR heuristic. Without this,
it's not clear what the cost between two states should be. Penalizing
time may not be the metric we want to optimize for. Choosing the lowest
cost over all possible time horizons does not guarantee completeness.
These are crucial points.

Many scenarios include time in the state anyway. For example: time-varying
dynamics or obstacles and constraints. 

Let $\left\langle x,k\right\rangle \in\mathbb{R}^{n}\times\mathbb{R}_{0+}$
be the space of the RRT state.


\subsubsection{Primitives}

$Steer(s_{1},s_{2})$

$T=k\left(s_{2}\right)-k\left(s_{1}\right)$

if $T\leq0$, return NullAction

follow LQR gain matrices around $x\left(s_{1}\right)$ with goal $x\left(s_{2}\right)$
, time horizon $T$

$Distance(s_{1},s_{2})$

use LQR cost-to-go


\subsubsection{Optimality Proof Sketch}

LQR solves a relaxed version of the problem -- no obstacle and no
actuation constraints. This is directly analogous to a Euclidean distance
metric being a relaxed version of the shortest path in the kinematic
case.


\section{Application to General Dynamical Systems}

Consider a discrete-time dynamical system in the form 
\begin{equation}
x_{k+1}=f\left(x_{k},u_{k}\right)\label{eq:dynamics}
\end{equation}


with additive cost function
\begin{equation}
J\left(\mathbf{u},\mathbf{x}\right)=\sum_{k=0}^{T}g\left(x_{k},u_{k}\right)\label{eq:cost_functional}
\end{equation}


and starting point $x_{0}$. The state vector, $x$, is $n$-dimensional
and the control vector, $u$, is $m$-dimensional. We aim to find
a sequence $ $$\mathbf{u}=\left\{ u_{0},\cdots,u_{T}\right\} $ which
induces a trajectory $\mathbf{x}=\left\{ x_{1},\cdots,x_{T}\right\} $
satisfying the dynamics (\ref{eq:dynamics}) such that $ $$C$ is
minimized according to (\ref{eq:cost_functional}).

The real cost of moving from point $x'$ to $x''$ is
\begin{multline*}
C\left(x,x'\right)=\min_{\mathbf{u}}J\left(\mathbf{u},\mathbf{x}\right)\\
\mbox{subject to (\ensuremath{\ref{eq:dynamics}}), \ensuremath{x_{0}=x}, \ensuremath{x_{T}=x'}}
\end{multline*}


Note that the minimization happens over control sequences $\mathbf{u}$
of a fixed time lengths, according to 

We approximate $C\left(x',x''\right)$ by taking a first-order approximation
of the dynamics and a second-order approximation of the cost and applying
LQR control. In general, the approximated dynamics and cost are of
the following form

\begin{align}
x_{k+1} & \approx Ax_{k}+Bu_{k}+c\label{eq:dynamics_approx}\\
J\left(\mathbf{u},\mathbf{x}\right) & \approx\sum_{k=0}^{T}x_{k}^{T}Qx_{k}+u_{k}^{T}Ru_{k}+2q^{T}x_{k}+2r^{T}u_{k}+d\label{eq:cost_approx}
\end{align}


$A$ and $Q$ are $n\times n$, $B$ is $m\times n$, $R$ is $n\times n$.
$c$ and $q$ are $n\times1$, $r$ is $m\times1$ and $d$ is a scalar.

\begin{align*}
A & =\left.\frac{\partial f}{\partial x}\right|_{x^{*},u^{*}}\\
B & =\left.\frac{\partial f}{\partial u}\right|_{x^{*},u^{*}}\\
c & =-Ax^{*}-Bu^{*}+f\left(x^{*},u^{*}\right)
\end{align*}


$x^{*}$, $u^{*}$ is the point about which the linearization is performed.
Typically $u^{*}$ is taken to be $\mathbf{0}$ and $x^{*}=x'$

Equations \ref{eq:dynamics_approx} and \ref{eq:cost_approx} are
the truncated Taylor expansions of $f$ and $g$. The dynamics $f$
must be once-differentiable and addition cost $g$ must be twice-differentiable.


\subsection{Reduction to the Previous Problem}

It is possible to transform the problem specified with \ref{eq:dynamics_approx}
and \ref{eq:cost_approx} into LQR form (where there is only an $A$,
$B$, $Q$, $R$ matrix) using the following:

\begin{align*}
\underbrace{\left[\begin{matrix}x_{k+1}\\
1
\end{matrix}\right]}_{\hat{x}_{k+1}} & =\underbrace{\left(\begin{matrix}A & c-R^{-1}r\\
0 & 1
\end{matrix}\right)}_{\hat{A}}\underbrace{\left[\begin{matrix}x_{k}\\
1
\end{matrix}\right]}_{\hat{x}_{k}}+\underbrace{\left(\begin{matrix}B\\
0
\end{matrix}\right)}_{\hat{B}}\hat{u}_{k}\\
C\left(\mathbf{u},\mathbf{x}\right) & =\sum_{k=0}^{T}\hat{x}_{k}^{T}\hat{Q}\hat{x}_{k}+\hat{u}_{k}^{T}R\hat{u}_{k}
\end{align*}


with $\hat{u}_{k}=u_{k}+R^{-1}r$ and $\hat{Q}=\left(\begin{matrix}Q & q\\
q^{T} & d
\end{matrix}\right)$ . 

\begin{comment}
$\left(\begin{matrix}x\\
1
\end{matrix}\right)^{T}\left(\begin{matrix}Q & q\\
q^{T} & \eta_{1}
\end{matrix}\right)\left(\begin{matrix}x\\
1
\end{matrix}\right)=\left(\begin{matrix}x\\
1
\end{matrix}\right)^{T}\left(\begin{matrix}Qx+q\\
q^{T}x+\eta_{1}
\end{matrix}\right)=x^{T}Qx+x^{T}q+q^{T}x+d$
\end{comment}


The $\hat{A}$, $\hat{B}$, $\hat{Q}$, and $R$ matrices specify
a linear dynamical system with quadratic costs to which an optimal
solution can be found with LQR. 


\subsection{Nuances and Subtleties}


\subsubsection{Non-exact steering}

rewiring and propagating dynamics


\subsubsection{Uncontrollable Dynamics}

The linearized system may be uncontrollable -- the $A$ and $B$ matrices
are such that it's not possible to control all the modes of the system.
This is the case, for example, for a cart with two inverted pendulums
of the same length linearized about the upward-pointing fixed point.
The control input to the system affects both linearized pendulums
in the same way, so it's not possible to independently stabilize them.
For the infinite-horizon LQR control problem, there is no solution.
For the finite-horizon problem, there is a solution, though it might
not be possible to go to any arbitrary location. If the system linearized
at $x'$ cannot reach $x''$, then $C\left(x',x''\right)$ needs to
be defined in another way.

is Therefore using the LQR cost metric cannot approximate the cost 

\begin{comment}
Typical RTT{*} the cost metric is an underestimate of the true cost,
since the metric does not take into consideration obstacles. The true
cost, for example, might be infinite if there is no feasible path,
but the Euclidian metric will always be finite.

If the system is uncontrollable as is linearized by a single point,
then the LQR cost will be infinite while the true cost is not.
\end{comment}



\subsubsection{Indefinite Cost}

\begin{comment}
What if $\hat{Q}$ is indefinite? 

Options:

1. Use sequential QP techniques like shifting eigenvalues to make
$\hat{Q}$ definite. 

http://www.cs.berkeley.edu/\textasciitilde{}pabbeel/cs287-fa11/slides/NonlinearOptimizationForOptimalControl.pdf
(Page 11 bottom slide)

2. LQR with indefnite weighting matrices

Chapter 9 of:

http://epubs.siam.org.libproxy.mit.edu/doi/book/10.1137/1.9781611970760

(same book) 

http://books.google.com/books?id=bD\_83idGZ2cC\&lpg=PA211\&ots=q3U7u4rmNc\&dq=indefinite\%20LQR\&pg=PA211\#v=onepage\&q\&f=false

http://www.tandfonline.com/doi/abs/10.1080/00207178408933184\#preview
\end{comment}



\subsubsection{Actuation Constraints}

The LQR framework does not permit actuation constraints.

\begin{comment}
Say the LQR solution returns a control action $u\not\in\mathcal{U}$
(the set of permitted actions). Simply choosing a $u'=\alpha u$ such
that $u'\in\mathcal{U}$ while minimizing $\alpha^{2}$ (so that $u'$
is close to $u$) intuitively will not explore the space as desired
-- the state won't move along the same direction in general.

To clarify:

if the state of the system is $x_{k}$, then the next state is $x_{k+1}=Ax_{k}+Bu$,
which presumably moves the system toward $x_{rand}$. The state $x_{k+1}'=Ax_{k}+Bu'$
does not, in general, move the system directly toward $x_{rand}$.
\end{comment}



\subsubsection{Asymmetric Cost}

\begin{comment}
http://math.stackexchange.com/a/23397/2256
\end{comment}



\section{Related Work}

vdB, Glassman, Perez


\section{Results}


\subsection{Linear Domain}

spaceship no orientation


\subsection{Non-linear Domain}

spaceship orientation


\subsection{Results}

Quick mention of performance (or not). Picture of tree, cost over
iteration


\section{Discussion and Conclusion and Future Work}

Code available. Spatial Data structure future


\section{Acknowledgments}


\section{References}
\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}
